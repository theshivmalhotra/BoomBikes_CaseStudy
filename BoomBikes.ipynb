{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #numerical analysis library\n",
    "import numpy as np #data analysis library\n",
    "import matplotlib.pyplot as plt #Visualization library\n",
    "import seaborn as sns #visualization library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the head of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the various aspects of the data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "#importing dataset through pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "#This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = data.pop('cnt')#putting the dependent or target variable in another variable\n",
    "data.insert(0, 'cnt', first_column)#inserting the new column in new dataset\n",
    "data=data.drop(['casual','registered','instant','mnth','weekday'],axis=1)#dropping all the columns based on domain knowledge, irrelavant on basis on problem statement & data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()\n",
    "#calculating some statistical data like percentile, mean and std of the numerical values of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data)#pairplot command\n",
    "plt.show()\n",
    "#ploting multiple scatterplot for continous variable to understand there relevance relative to target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col=['season','yr','holiday','workingday','weathersit']#segrating categorical driver from dataframe for plotting purpose\n",
    "cont_col=['temp','atemp','hum','windspeed']\n",
    "\n",
    "season_dict = {1:'spring', 2:'summer', 3:'fall', 4:'winter'}#creating season a dictionary to replace 0 or 1 with corresponding categorical value\n",
    "yr_dict={0:'2018',1:'2019'}#creating yr a dictionary to replace 0 or 1 with corresponding categorical value\n",
    "holiday_dict={0:'Non-Holiday',1:'Holiday'}#creating holiday a dictionary to replace 0 or 1 with corresponding categorical value\n",
    "workingday_dict={0:\"Non-Working\",1:'Working'}#creating working a dictionary to replace 0 or 1 with corresponding categorical value\n",
    "weathersit_dict={1:'Clear or No Cloud',2:'Mist Cloud',3:'Light Snow or Rain',4:'Heavy Rain or Snow'}#creating a working dictionary to replace 0 or 1 with corresponding categorical value\n",
    "data1=pd.DataFrame()#creating a new dataframe & adding all the above categorical values into this\n",
    "data1['season']=data['season'].replace(season_dict)\n",
    "data1['holiday']=data['holiday'].replace(holiday_dict)\n",
    "data1['yr']=data['yr'].replace(yr_dict)\n",
    "data1['workingday']=data['workingday'].replace(workingday_dict)\n",
    "data1['weathersit']=data['weathersit'].replace(weathersit_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in cat_col:\n",
    "    plt.figure(figsize=(12,6))#fixing the figure size of the plot\n",
    "    sns.countplot(data1[i])#running command for multiple count plot for categorical variable\n",
    "    plt.title(\"Countplot for \"+i,size=17) #setting the title for each count plot\n",
    "    plt.xlabel(i,fontsize=13)#fixing the label size of x axis driver\n",
    "    plt.ylabel('Count',fontsize=13)##fixing the label size of y axis Count\n",
    "    plt.show()\n",
    "\n",
    "#plotting multiplecount plots for categorical variables to understand their relevance & get meaningful insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cont_col:\n",
    "    plt.figure(figsize=(10,5))#fixing the figure size of the plot\n",
    "    plt.boxplot(data[i])#running command for multiple box plot for categorical variable\n",
    "    plt.title(\"Boxplot for \"+i,size=17) #setting the title for each box plot\n",
    "    plt.xlabel(i,fontsize=15)#fixing the label size of x axis driver\n",
    "    plt.show()\n",
    "\n",
    "#ploting multiple boxplots for continous drivers to have an understanding of outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dummy variables for the feature season & weathersit and store it in a new variable - 'status' & 'status1'\n",
    "status = pd.get_dummies(data1['season'])\n",
    "status1 = pd.get_dummies(data1['weathersit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the first column from status df using 'drop_first = True'\n",
    "\n",
    "status = pd.get_dummies(data1['season'], drop_first = True)\n",
    "status1 = pd.get_dummies(data1['weathersit'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to the original data dataframe\n",
    "\n",
    "data = pd.concat([data, status], axis = 1)\n",
    "data = pd.concat([data, status1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the head of our dataframe.\n",
    "data=data.drop([\"Unnamed: 16\",\"Unnamed: 17\"],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop season & weatherit as we have created the dummies for it & holiday as we already have working day which tell 0 as weekend or holiday\n",
    "data=data.drop(['weathersit','season','dteday'],axis=1)#dropping all the columns based on domain knowledge, irrelavant on basis on problem statement & data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,5)) #fixing the figure size of the plot\n",
    "sns.heatmap(data.corr(),annot=True,cmap='Greens',fmt='.1%',cbar=False)#running command for heatmap for correlation of continous variable\n",
    "plt.show()\n",
    "#correlation matrix formed through heatmap to understand the correlation of continous variables & there relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['atemp'],axis=1)#dropping atemp as it has a high correlation with temp & atemp is a feeling temperature which will be less accurate than temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #importing relevant libraries for model building\n",
    "\n",
    "# We specify this so that the train and test data set always have the same rows, respectively\n",
    "np.random.seed(0)\n",
    "df_train, df_test = train_test_split(data, train_size = 0.7, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler #importing relevant libraries for scaling the model\n",
    "scaler = MinMaxScaler() #adding the scaling method in a variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\n",
    "num_vars = ['temp','hum','windspeed','cnt']\n",
    "df_train[num_vars] = scaler.fit_transform(df_train[num_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the correlation coefficients to see which variables are highly correlated\n",
    "\n",
    "plt.figure(figsize = (16, 10))\n",
    "sns.heatmap(df_train.corr(),annot=True,cmap='Greens',fmt='.1%',cbar=False )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing into X and Y sets for the model building\n",
    "y_train = df_train.pop('cnt')\n",
    "X_train = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE #importing relevant libraries to use recursive feature elimination\n",
    "from sklearn.linear_model import LinearRegression #importing relevant libraries for Linear Regression modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running RFE with the output number of the variable equal to 8\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "rfe = RFE(lm, step=8)    # running RFE\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "list(zip(X_train.columns,rfe.support_,rfe.ranking_))#checking the list with the top drivers & proceeding ahead with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant or intercept & selecting temp as first feature or driver it has the highest correlation with dependent variable\n",
    "X_train_lm = sm.add_constant(X_train[['temp']])\n",
    "\n",
    "# Create a first fitted model\n",
    "lr = sm.OLS(y_train, X_train_lm).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the parameters obtained\n",
    "lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the data with a scatter plot and the fitted regression line\n",
    "plt.scatter(X_train_lm.iloc[:, 1], y_train)\n",
    "plt.plot(X_train_lm.iloc[:, 1], 0.169798 +0.639952*X_train_lm.iloc[:, 1], 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of the linear regression model obtained\n",
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning additional feature variables to X\n",
    "X_train_lm = X_train[['Light Snow or Rain','yr','windspeed','temp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear model\n",
    "\n",
    "import statsmodels.api as sm\n",
    "X_train_lm = sm.add_constant(X_train_lm)\n",
    "\n",
    "lr = sm.OLS(y_train, X_train_lm).fit()\n",
    "\n",
    "lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary of the model\n",
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the p-values in the above result we have it looks like the variables are really significant\n",
    "And also our adjusted R2 has increase from 41% to 73%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning additional feature variables to X on the basis of the REF & above correlation matrix \n",
    "X_train_lm_1 = X_train[['yr','temp','windspeed','spring','Light Snow or Rain','Mist Cloud']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear model with new features\n",
    "\n",
    "import statsmodels.api as sm\n",
    "X_train_lm_1 = sm.add_constant(X_train_lm_1)\n",
    "lr1 = sm.OLS(y_train, X_train_lm_1).fit()\n",
    "lr1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary of the model\n",
    "\n",
    "print(lr1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the p-values in the above result we have it looks like of the variables are really significant\n",
    "And also our adjusted R2 has increase from 73% to 81%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning additional feature variables to X on the basis of the REF & above correlation matrix \n",
    "X_train_lm_2 = X_train[['yr','holiday','windspeed','spring','summer','winter','Light Snow or Rain','Mist Cloud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear model with new features\n",
    "\n",
    "import statsmodels.api as sm\n",
    "X_train_lm_2 = sm.add_constant(X_train_lm_2)\n",
    "\n",
    "lr2 = sm.OLS(y_train, X_train_lm_2).fit()\n",
    "\n",
    "lr2.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary of the model\n",
    "print(lr2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the p-values in the above result we have it looks like the variables are really significant apart from working day as it has p value higher than 0.5\n",
    "But our adjusted R2 has stopped improving it means we will stick to our previous model lr1 or feature in X_train_lm_1  that are ['Light Snow or Rain','yr','windspeed','temp','Mist Cloud','spring']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Analysis of the train data\n",
    "\n",
    "So, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the VIF values of the feature variables. \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cnt = lr1.predict(X_train_lm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the error terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((y_train - y_train_cnt), bins = 20)\n",
    "fig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \n",
    "plt.xlabel('Errors', fontsize = 18)                         # X-label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above figure our model residual is perfectly aligned at mean=0 which show its an acceptable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Predictions Using the Final Model\n",
    "num_vars = ['temp','hum','windspeed','cnt']\n",
    "df_test[num_vars] = scaler.transform(df_test[num_vars])# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing df_test set into X_test and y_test\n",
    "y_test = df_test.pop('cnt')\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding constant variable to test dataframe\n",
    "X_test_1 = sm.add_constant(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X_test_m2 dataframe by dropping variables from X_test_m1\n",
    "\n",
    "X_test_m2 = X_test_1.drop([ 'workingday','hum','holiday','summer','winter'], axis = 1)\n",
    "X_test_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred to understand the spread\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(y_test, y_pred_m2)\n",
    "fig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \n",
    "plt.xlabel('y_test', fontsize = 18)                          # X-label\n",
    "plt.ylabel('y_pred', fontsize = 16)  \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the lr1 which has model feature in X_train_lm_1  that are ['Light Snow or Rain','yr','windspeed','temp','Mist Cloud','spring']\n",
    "from sklearn.metrics import r2_score\n",
    "y_pred_m2 = lr1.predict(X_test_m2)\n",
    "r2_score(y_true=y_test,y_pred=y_pred_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that our R2 score on the test model is 79% which is quite similar to our training model of lr1(below) that is 81%. which shows that our model is quite stable & all the chosen driver are significant one's related to our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our above choosen model lr1 has following coefficients with R2 as 81%:-\n",
    "const                 0.320383- Intercept which states the value of our dependent 'cnt' when all our independent variables are kept as 0\n",
    "\n",
    "yr                    0.236372-  year (0: 2018, 1:2019) which means \n",
    "\n",
    "temp                  0.362729 - temperature in Celsius as it has positive coeff which states that if the temperature is going to increase, our count of total rental bikes including both casual and registered is going to increase.\n",
    "\n",
    "windspeed            -0.157135 - windspeed has negative coeff which states that if the windspeed is going to decrease, our count of total rental bikes including both casual and registered is going to increase.\n",
    "\n",
    "spring               -0.153830 - spring season has negative coeff which states that as less as spring season is going to be or as ealry spring season is going to over or end, our count of total rental bikes including both casual and registered is going to increase.\n",
    "\n",
    "Light Snow or Rain   -0.271313 - Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds has negative coeff which states that the less this type of weather condition is going to be,  is the more our count of total rental bikes including both casual and registered is going to increase.\n",
    "\n",
    "Mist Cloud           -0.075476 - Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist has negative coeff which states that the less this type of weather condition is going to be,  is the more our count of total rental bikes including both casual and registered is going to increase.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6aae9e3b5b1a6982bb75e9b88e9c5453921bcfe9f533234dee128d1b4ee2325"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
